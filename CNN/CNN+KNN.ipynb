{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# from torch.optim import AdamW\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import KDTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('./df1.csv')\n",
    "df1['abstract'] = df1['abstract'].astype(str)\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载Word2Vec模型\n",
    "model_path = \"./GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# NLTK资源下载\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 分词和预处理函数\n",
    "def tokenize_and_process(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word.lower() for word in words]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "# 对DataFrame中的abstract列进行分词和预处理\n",
    "df1['tokenized_abstract'] = df1['abstract'].apply(tokenize_and_process)\n",
    "\n",
    "# 函数来将单词转换为Word2Vec向量，如果模型中没有该词，则使用UNK向量\n",
    "def word_to_vec(word, model):\n",
    "    return model[word] if word in model.key_to_index else model['UNK']\n",
    "\n",
    "# 函数来将句子的分词列表转换为Word2Vec向量列表\n",
    "def tokens_to_vectors(tokens, model):\n",
    "    return [word_to_vec(token, model) for token in tokens]\n",
    "\n",
    "# 应用函数将tokenized_abstract列的分词列表转换为Word2Vec向量列表\n",
    "df1['word_vectors'] = df1['tokenized_abstract'].apply(lambda tokens: tokens_to_vectors(tokens, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 找出word_vectors中最长的长度\n",
    "MAX_SEQUENCE_LENGTH = df1['word_vectors'].apply(len).max()\n",
    "UNK_VECTOR = word2vec_model['UNK']  # 获取'UNK'的向量表示\n",
    "# 使用这个长度作为填充或截断的基础\n",
    "def pad_or_truncate_vectors(word_vectors, max_length, unk_vector):\n",
    "    \"\"\"\n",
    "    如果word_vectors长度小于max_length，则用unk_vector填充；\n",
    "    如果word_vectors长度大于max_length，则截断。\n",
    "    \"\"\"\n",
    "    # 获取当前word vectors的长度\n",
    "    sequence_length = len(word_vectors)\n",
    "    \n",
    "    # 如果当前长度小于最大长度，进行填充\n",
    "    if sequence_length < max_length:\n",
    "        padding = [unk_vector] * (max_length - sequence_length)\n",
    "        word_vectors.extend(padding)\n",
    "    # 如果当前长度大于最大长度，进行截断\n",
    "    elif sequence_length > max_length:\n",
    "        word_vectors = word_vectors[:max_length]\n",
    "    \n",
    "    return word_vectors\n",
    "\n",
    "# 应用函数pad_or_truncate_vectors到每一行的word_vectors列\n",
    "df1['padded_word_vectors'] = df1['word_vectors'].apply(\n",
    "    lambda x: pad_or_truncate_vectors(x, MAX_SEQUENCE_LENGTH, UNK_VECTOR))\n",
    "\n",
    "# 检查结果\n",
    "df1['padded_word_vectors'].apply(len)  # 每个向量的长度都应该是MAX_SEQUENCE_LENGTH\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 准备特征：将padded_word_vectors的列表转换为NumPy数组\n",
    "X = np.array(df1['padded_word_vectors'].tolist())\n",
    "\n",
    "# 准备标签：获取所有标签列\n",
    "label_columns = df1.columns.difference(['abstract', 'tokenized_abstract', 'word_vectors', 'padded_word_vectors'])\n",
    "y = df1[label_columns].values\n",
    "\n",
    "# 分割数据集为训练集和测试集，这里使用20%的数据作为测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 输出分割后的数据集维度，仅用于确认\n",
    "print(f'Training set shape: {X_train.shape, y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape, y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from sklearn.neighbors import NearestNeighbors  # 这里导入NearestNeighbors\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "    # 预测值大于0.3的被认为是正类\n",
    "    y_pred = K.cast(K.greater(y_pred, 0.3), K.floatx())\n",
    "    \n",
    "    # 计算真正例、假正例和假负例\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)\n",
    "    \n",
    "    # 计算精确度和召回率\n",
    "    precision = K.sum(true_positives) / (K.sum(predicted_positives) + K.epsilon())\n",
    "    recall = K.sum(true_positives) / (K.sum(possible_positives) + K.epsilon())\n",
    "    \n",
    "    # 计算micro-F1分数\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "# 模型参数\n",
    "max_sequence_length = 237  # 句子的最大长度\n",
    "embedding_dim = 300  # 词嵌入的维度\n",
    "num_labels = 145  # 标签的数量\n",
    "\n",
    "# 模型输入\n",
    "input_ = Input(shape=(max_sequence_length, embedding_dim))\n",
    "\n",
    "# 卷积层：使用多个不同大小的滤波器来提取特征\n",
    "# 可以通过交叉验证调整filters和kernel_size\n",
    "conv = Conv1D(filters=128, kernel_size=5, activation='relu')(input_)\n",
    "\n",
    "# 全局最大池化层：为了捕捉最重要的特征\n",
    "gmp = GlobalMaxPooling1D(name='gmp')(conv)\n",
    "\n",
    "# Dropout层：减少过拟合\n",
    "dropout = Dropout(0.5)(gmp)\n",
    "\n",
    "# 全连接层：进一步处理特征\n",
    "# 可以通过交叉验证调整units\n",
    "dense = Dense(128, activation='relu')(dropout)\n",
    "\n",
    "# 输出层：使用sigmoid函数，适用于多标签分类\n",
    "# 输出层：使用sigmoid函数，适用于多标签分类\n",
    "output = Dense(num_labels, activation='sigmoid', name='output')(dense)\n",
    "# 构建模型\n",
    "model = Model(inputs=input_, outputs=[output,gmp])\n",
    "\n",
    "# 编译模型：使用二元交叉熵作为损失函数，适用于多标签分类\n",
    "model.compile(optimizer='adam',    loss={'output': 'binary_crossentropy', 'gmp': None}, metrics={'output': micro_f1})\n",
    "\n",
    "# 打印模型结构\n",
    "print(model.summary())\n",
    "\n",
    "# 训练模型：使用训练数据和验证数据\n",
    "# 这里假设X_train和y_train已经准备好，且与模型输入输出匹配\n",
    "# 可以通过交叉验证调整batch_size和epochs\n",
    "model.fit(X_train, {'output': y_train}, batch_size=32, epochs=20, validation_split=0.1)\n",
    "\n",
    "# 在测试集上评估模型性能\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# evaluation_results[0] 是整体的损失值\n",
    "# evaluation_results[1] 是主输出层的损失值\n",
    "# evaluation_results[2] 是主输出层的micro-F1分数\n",
    "\n",
    "# 打印测试集上的性能\n",
    "print(f'Test loss (overall): {evaluation_results[0]}')\n",
    "print(f'Test loss (main output): {evaluation_results[1]}')\n",
    "print(f'Test micro-F1 score (main output): {evaluation_results[2]}')\n",
    "\n",
    "\n",
    "# Extract features from the training set using the trained model\n",
    "# This will be used for constructing the datastore for k-NN\n",
    "train_features = model.predict(X_train)[1]\n",
    "\n",
    "# Construct the datastore for k-NN using the extracted features and the known labels\n",
    "datastore = NearestNeighbors(n_neighbors=5).fit(train_features)\n",
    "\n",
    "# Define k-NN inference function\n",
    "def knn_inference(model, datastore, x_test, k=5, temperature=1.0, lambda_factor=0.5):\n",
    "    cnn_test_predictions, test_features = model.predict(x_test)\n",
    "    \n",
    "    knn_test_predictions = np.zeros(cnn_test_predictions.shape)\n",
    "    \n",
    "    for i, feature in enumerate(test_features):\n",
    "        distances, indices = datastore.kneighbors([feature], n_neighbors=k)\n",
    "        weights = np.exp(-np.array(distances) / temperature)\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        for idx, w in zip(indices[0], weights[0]):\n",
    "            knn_test_predictions[i] += w * y_train[idx]\n",
    "    \n",
    "    knn_test_predictions /= np.max(knn_test_predictions, axis=1, keepdims=True)\n",
    "    \n",
    "    # Combine CNN model's output with k-NN's prediction to get the final prediction\n",
    "    final_predictions = lambda_factor * cnn_test_predictions + (1 - lambda_factor) * knn_test_predictions\n",
    "    return final_predictions\n",
    "\n",
    "# Perform inference with k-NN on the test set\n",
    "final_predictions = knn_inference(model, datastore, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "\n",
    "# 定义阈值\n",
    "threshold = 0.3\n",
    "\n",
    "# 将概率转换为二进制标签\n",
    "y_pred_binary = (final_predictions >= threshold).astype(int)\n",
    "\n",
    "# 计算micro-F1分数\n",
    "micro_f1_score = f1_score(y_test, y_pred_binary, average='micro')\n",
    "\n",
    "# 打印micro-F1分数\n",
    "print(f'Micro-F1 score for final predictions: {micro_f1_score}')\n",
    "\n",
    "# 计算测试误差（均方误差）\n",
    "test_error = mean_squared_error(y_test, final_predictions)\n",
    "\n",
    "# 打印测试误差\n",
    "print(f'Test mean squared error: {test_error}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
