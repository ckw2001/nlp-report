{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.10.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: lit in /root/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: gensim in /root/miniconda3/lib/python3.8/site-packages (4.3.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /root/miniconda3/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /root/miniconda3/lib/python3.8/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /root/miniconda3/lib/python3.8/site-packages (from gensim) (1.24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /root/miniconda3/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.8/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/miniconda3/lib/python3.8/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.8/site-packages (from nltk) (1.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (1.24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /root/miniconda3/lib/python3.8/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.8/site-packages (4.35.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.8/site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: keras in /root/miniconda3/lib/python3.8/site-packages (2.13.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tensorflow in /root/miniconda3/lib/python3.8/site-packages (2.13.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (1.51.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (4.22.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /root/miniconda3/lib/python3.8/site-packages (from tensorflow) (1.24.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /root/miniconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /root/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /root/miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /root/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 00:56:10.415192: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-24 00:56:10.485355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-24 00:56:11.645033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# from torch.optim import AdamW\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import KDTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv('./df1.csv',nrows=10000)\n",
    "df1['abstract'] = df1['abstract'].astype(str)\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 加载Word2Vec模型\n",
    "model_path = \"./GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to autodl-fs/nltk_data2...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to autodl-fs/nltk_data2...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to autodl-fs/nltk_data2...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确保这是你的nltk_data目录所在的路径\n",
    "nltk_data_path = 'autodl-fs/nltk_data2'\n",
    "\n",
    "# 添加到NLTK的数据路径\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 分词和预处理函数\n",
    "def tokenize_and_process(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word.lower() for word in words]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "# 对DataFrame中的abstract列进行分词和预处理\n",
    "df1['tokenized_abstract'] = df1['abstract'].apply(tokenize_and_process)\n",
    "\n",
    "# 函数来将单词转换为Word2Vec向量，如果模型中没有该词，则使用UNK向量\n",
    "def word_to_vec(word, model):\n",
    "    return model[word] if word in model.key_to_index else model['UNK']\n",
    "\n",
    "# 函数来将句子的分词列表转换为Word2Vec向量列表\n",
    "def tokens_to_vectors(tokens, model):\n",
    "    return [word_to_vec(token, model) for token in tokens]\n",
    "\n",
    "# 应用函数将tokenized_abstract列的分词列表转换为Word2Vec向量列表\n",
    "df1['word_vectors'] = df1['tokenized_abstract'].apply(lambda tokens: tokens_to_vectors(tokens, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: ((8000, 217, 300), (8000, 145))\n",
      "Test set shape: ((2000, 217, 300), (2000, 145))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 找出word_vectors中最长的长度\n",
    "MAX_SEQUENCE_LENGTH = df1['word_vectors'].apply(len).max()\n",
    "UNK_VECTOR = word2vec_model['UNK']  # 获取'UNK'的向量表示\n",
    "# 使用这个长度作为填充或截断的基础\n",
    "def pad_or_truncate_vectors(word_vectors, max_length, unk_vector):\n",
    "    \"\"\"\n",
    "    如果word_vectors长度小于max_length，则用unk_vector填充；\n",
    "    如果word_vectors长度大于max_length，则截断。\n",
    "    \"\"\"\n",
    "    # 获取当前word vectors的长度\n",
    "    sequence_length = len(word_vectors)\n",
    "    \n",
    "    # 如果当前长度小于最大长度，进行填充\n",
    "    if sequence_length < max_length:\n",
    "        padding = [unk_vector] * (max_length - sequence_length)\n",
    "        word_vectors.extend(padding)\n",
    "    # 如果当前长度大于最大长度，进行截断\n",
    "    elif sequence_length > max_length:\n",
    "        word_vectors = word_vectors[:max_length]\n",
    "    \n",
    "    return word_vectors\n",
    "\n",
    "# 应用函数pad_or_truncate_vectors到每一行的word_vectors列\n",
    "df1['padded_word_vectors'] = df1['word_vectors'].apply(\n",
    "    lambda x: pad_or_truncate_vectors(x, MAX_SEQUENCE_LENGTH, UNK_VECTOR))\n",
    "\n",
    "# 检查结果\n",
    "df1['padded_word_vectors'].apply(len)  # 每个向量的长度都应该是MAX_SEQUENCE_LENGTH\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 准备特征：将padded_word_vectors的列表转换为NumPy数组\n",
    "X = np.array(df1['padded_word_vectors'].tolist())\n",
    "\n",
    "# 准备标签：获取所有标签列\n",
    "label_columns = df1.columns.difference(['abstract', 'tokenized_abstract', 'word_vectors', 'padded_word_vectors'])\n",
    "y = df1[label_columns].values\n",
    "\n",
    "# 分割数据集为训练集和测试集，这里使用20%的数据作为测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 输出分割后的数据集维度，仅用于确认\n",
    "print(f'Training set shape: {X_train.shape, y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape, y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# 指定学习率\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 创建优化器实例\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    # 预测值大于0.3的被认为是正类\n",
    "    threshold = 0.3\n",
    "    y_pred_thresholded = tf.cast(tf.greater(y_pred, threshold), tf.float32)\n",
    "    \n",
    "    # 计算准确度\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred_thresholded), tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "    # 预测值大于0.3的被认为是正类\n",
    "    y_pred = K.cast(K.greater(y_pred, 0.3), K.floatx())\n",
    "    \n",
    "    # 计算真正例、假正例和假负例\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)\n",
    "    \n",
    "    # 计算精确度和召回率\n",
    "    precision = K.sum(true_positives) / (K.sum(predicted_positives) + K.epsilon())\n",
    "    recall = K.sum(true_positives) / (K.sum(possible_positives) + K.epsilon())\n",
    "    \n",
    "    # 计算micro-F1分数\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "# 模型参数\n",
    "max_sequence_length = 237  # 句子的最大长度\n",
    "embedding_dim = 300  # 词嵌入的维度\n",
    "num_labels = 145  # 标签的数量\n",
    "\n",
    "# 模型输入\n",
    "input_ = Input(shape=(max_sequence_length, embedding_dim))\n",
    "\n",
    "# 卷积层：使用多个不同大小的滤波器来提取特征\n",
    "# 可以通过交叉验证调整filters和kernel_size\n",
    "conv = Conv1D(filters=128, kernel_size=5, activation='relu')(input_)\n",
    "\n",
    "# 全局最大池化层：为了捕捉最重要的特征\n",
    "gmp = GlobalMaxPooling1D()(conv)\n",
    "\n",
    "# Dropout层：减少过拟合\n",
    "dropout = Dropout(0.5)(gmp)\n",
    "\n",
    "# 全连接层：进一步处理特征\n",
    "# 可以通过交叉验证调整units\n",
    "dense = Dense(128, activation='relu')(dropout)\n",
    "\n",
    "# 输出层：使用sigmoid函数，适用于多标签分类\n",
    "output = Dense(num_labels, activation='sigmoid')(dense)\n",
    "\n",
    "# 构建模型\n",
    "model = Model(inputs=input_, outputs=output)\n",
    "\n",
    "# 编译模型：使用二元交叉熵作为损失函数，适用于多标签分类\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[micro_f1])\n",
    "\n",
    "# 打印模型结构\n",
    "print(model.summary())\n",
    "\n",
    "# 训练模型：使用训练数据和验证数据\n",
    "# 这里假设X_train和y_train已经准备好，且与模型输入输出匹配\n",
    "# 可以通过交叉验证调整batch_size和epochs\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=30, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估模型性能\n",
    "test_loss, test_micro_f1 = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 打印测试集上的性能\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test loss: {test_micro_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'learning_rate': 0.001, 'filters': 128, 'kernel_size': (3,), 'dense_units': 64}, batch_size: 64, epochs: 30\n",
      "Testing parameters: {'learning_rate': 0.0001, 'filters': 64, 'kernel_size': (5,), 'dense_units': 256}, batch_size: 128, epochs: 10\n",
      "Testing parameters: {'learning_rate': 0.001, 'filters': 64, 'kernel_size': (3,), 'dense_units': 128}, batch_size: 64, epochs: 20\n",
      "Testing parameters: {'learning_rate': 0.001, 'filters': 128, 'kernel_size': (7,), 'dense_units': 256}, batch_size: 128, epochs: 20\n",
      "Testing parameters: {'learning_rate': 0.01, 'filters': 64, 'kernel_size': (3,), 'dense_units': 64}, batch_size: 64, epochs: 20\n",
      "Best score: 0\n",
      "Best parameters: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "# 指定学习率\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "    # 预测值大于0.3的被认为是正类\n",
    "    y_pred = K.cast(K.greater(y_pred, 0.3), K.floatx())\n",
    "    \n",
    "    # 计算真正例、假正例和假负例\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)\n",
    "    \n",
    "    # 计算精确度和召回率\n",
    "    precision = K.sum(true_positives) / (K.sum(predicted_positives) + K.epsilon())\n",
    "    recall = K.sum(true_positives) / (K.sum(possible_positives) + K.epsilon())\n",
    "    \n",
    "    # 计算micro-F1分数\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "# 模型参数\n",
    "max_sequence_length = 217  # 句子的最大长度\n",
    "embedding_dim = 300  # 词嵌入的维度\n",
    "num_labels = 145  # 标签的数量\n",
    "\n",
    "\n",
    "# 构建模型的函数\n",
    "def build_model(learning_rate=1e-3, filters=128, kernel_size=5, dense_units=128):\n",
    "    input_ = Input(shape=(max_sequence_length, embedding_dim))\n",
    "    conv = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(input_)\n",
    "    gmp = GlobalMaxPooling1D()(conv)\n",
    "    dropout = Dropout(0.5)(gmp)\n",
    "    dense = Dense(dense_units, activation='relu')(dropout)\n",
    "    output = Dense(num_labels, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=[micro_f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 定义要搜索的超参数网格\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'filters': [64, 128, 256],\n",
    "    'kernel_size': [3, 5, 7],\n",
    "    'dense_units': [64, 128, 256],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# 要搜索的超参数的数量\n",
    "n_iter =10  # 你可以根据需要进行更多或更少的迭代\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # 随机选择超参数\n",
    "    params = {k: np.random.choice(v) for k, v in param_grid.items() if k not in ['batch_size', 'epochs']}\n",
    "\n",
    "    # 将 kernel_size 参数转换为元组\n",
    "    if 'kernel_size' in params:\n",
    "        params['kernel_size'] = (params['kernel_size'],)\n",
    "\n",
    "    batch_size = np.random.choice(param_grid['batch_size'])\n",
    "    epochs = np.random.choice(param_grid['epochs'])\n",
    "    print(f\"Testing parameters: {params}, batch_size: {batch_size}, epochs: {epochs}\")\n",
    "    # 构建和编译模型\n",
    "    model = build_model(**params)\n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # 获取验证集上的micro_f1得分\n",
    "    val_micro_f1 = np.max(history.history['val_micro_f1'])  # 假设'micro_f1'是编译模型时使用的名称\n",
    "    \n",
    "    # 如果找到了更好的得分，更新最佳得分和参数\n",
    "    if val_micro_f1 > best_score:\n",
    "        best_score = val_micro_f1\n",
    "        best_params = params\n",
    "        best_params['batch_size'] = batch_size\n",
    "        best_params['epochs'] = epochs\n",
    "        print(f\"New best score: {best_score}\")\n",
    "\n",
    "print(f\"Best score: {best_score}\")\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
